{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "context_length = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 300\n",
    "n_embd = 384\n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "n_heads = 6\n",
    "\n",
    "rope_embeddings = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before cleaning: ੈਟਰੋਲ ਤੇ ਡੀਜ਼ਲ ਦੇ ਭਾਅ 'ਚ ਕੀਤੇ ਗਏ ਅਥਾਹ ਵਾਧੇ ਅਤੇ ਪੰਜਾਬ ਸਰਕਾਰ ਵਲੋਂ ਚੋਣਾ ਦੌਰਾਨ ਗਰੀਬਾਂ ਤੇ ਮਜਦੂਰਾਂ ਨੂੰ 5-5 ...\n",
      "ਅੰਮਿ੍ਤਸਰ, 15 ਅਕਤੂਬਰ (ਹਰਮਿੰਦਰ ਸਿੰਘ)-ਪੰਜਾਬ ਦੇ ਕੈਬਨਿਟ ਮੰਤਰੀ ਨਵਜੋਤ ਸਿੰਘ ਸਿੱਧੂ ਦੇ ਿਖ਼ਲਾਫ਼ ਭਾਜਪਾ ਦੀ ਜ਼ਿਲ੍ਹਾ ਇਕਾਈ ਵਲੋਂ ਸਥਾਨਕ ਹਾਥੀ ਗੇਟ ਦੇ ਬਾਹਰਵਾਰ ਜ਼ੋਰਦਾਰ ਮੁਜ਼ਾਹਰਾ ਕੀਤਾ ਗਿਆ ਤੇ ਉਸ ਦਾ ਪੁਤਲਾ ਸਾੜਿਆ ਗਿਆ _ ਇਸ ਮੁਜ਼ਾਹਰੇ ਦੀ ਅਗਵਾਈ ...\n",
      "ਹਿਮਾਚਲ ਪ੍ਰਦੇਸ਼ ਤੋਂ ਆਈ ਸ਼ਰਧਾਲੂ ਔਰਤ ਦਾ ਲੁਟੇਰੇ ਪਰਸ ਖੋਹ ਕੇ ਫਰਾਰ\n",
      "ਅੰਮਿ੍ਤਸਰ, 15 ਅਕਤੂਬਰ (ਰੇਸ਼ਮ ਸਿੰਘ)-ਹਿਮਾਚਲ ਪ੍ਰਦੇਸ਼ ਦੇ ਇਲਾਕੇ ਪਾਉਂਟਾ ਸਾਹਿਬ ਤੋਂ ਆਪਣੇ ਪਰਿਵਾਰ ਸਮੇਤ ਸ੍ਰੀ ਹਰਿਮੰਦਰ ਸਾਹਿਬ ਦੇ ਦਰਸ਼ਨਾ\n",
      "\n",
      "Data after  cleaning: ੈਟਰੋਲ ਤੇ ਡੀਜ਼ਲ ਦੇ ਭਾਅ ਚ ਕੀਤੇ ਗਏ ਅਥਾਹ ਵਾਧੇ ਅਤੇ ਪੰਜਾਬ ਸਰਕਾਰ ਵਲੋਂ ਚੋਣਾ ਦੌਰਾਨ ਗਰੀਬਾਂ ਤੇ ਮਜਦੂਰਾਂ ਨੂੰ 5 5 \n",
      "ਅੰਮਿ੍ਤਸਰ, 15 ਅਕਤੂਬਰ ਹਰਮਿੰਦਰ ਸਿੰਘ ਪੰਜਾਬ ਦੇ ਕੈਬਨਿਟ ਮੰਤਰੀ ਨਵਜੋਤ ਸਿੰਘ ਸਿੱਧੂ ਦੇ ਿਖ਼ਲਾਫ਼ ਭਾਜਪਾ ਦੀ ਜ਼ਿਲ੍ਹਾ ਇਕਾਈ ਵਲੋਂ ਸਥਾਨਕ ਹਾਥੀ ਗੇਟ ਦੇ ਬਾਹਰਵਾਰ ਜ਼ੋਰਦਾਰ ਮੁਜ਼ਾਹਰਾ ਕੀਤਾ ਗਿਆ ਤੇ ਉਸ ਦਾ ਪੁਤਲਾ ਸਾੜਿਆ ਗਿਆ ਇਸ ਮੁਜ਼ਾਹਰੇ ਦੀ ਅਗਵਾਈ \n",
      "ਹਿਮਾਚਲ ਪ੍ਰਦੇਸ਼ ਤੋਂ ਆਈ ਸ਼ਰਧਾਲੂ ਔਰਤ ਦਾ ਲੁਟੇਰੇ ਪਰਸ ਖੋਹ ਕੇ ਫਰਾਰ\n",
      "ਅੰਮਿ੍ਤਸਰ, 15 ਅਕਤੂਬਰ ਰੇਸ਼ਮ ਸਿੰਘ ਹਿਮਾਚਲ ਪ੍ਰਦੇਸ਼ ਦੇ ਇਲਾਕੇ ਪਾਉਂਟਾ ਸਾਹਿਬ ਤੋਂ ਆਪਣੇ ਪਰਿਵਾਰ ਸਮੇਤ ਸ੍ਰੀ ਹਰਿਮੰਦਰ ਸਾਹਿਬ ਦੇ ਦਰਸ਼ਨਾ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_non_punjabi_chars(text):\n",
    "    punjabi_chars = r\"[\\u0A01-\\u0A7F\\u0A80-\\u0A8F,।0-9? \\n‘]\"  # Gurmukhi range\n",
    "    english_chars = r\"[a-zA-Z]\"  # English alphabet range\n",
    "    text =  re.sub(r\"[^\" + punjabi_chars +\"|\"+ english_chars + \"]+\", \" \", text) \n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "    return text\n",
    "\n",
    "# reading the punjabi corpus\n",
    "\n",
    "with open('data/pa.txt') as file:\n",
    "    punj_data = file.read()\n",
    "\n",
    "# Looking at random example of data sample before and after cleaning\n",
    "ind = random.randint(0, len(data)-500)\n",
    "\n",
    "print(f'Data before cleaning: {punj_data[ind:ind+500]}\\n')\n",
    "print(f'Data after  cleaning: {remove_non_punjabi_chars(punj_data[ind:ind+500])}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 126\n",
      "unique_charcters: \n",
      " ,0123456789?[।ਁਂਃ਄ਅਆਇਈਉਊ਌਍਎ਏਐਓਔਕਖਗਘਙਚਛਜਝਞਟਠਡਢਣਤਥਦਧਨ਩ਪਫਬਭਮਯਰਲਲ਼਴ਵਸ਼਷ਸਹ਼ਾਿੀੁੂ੃੄੆ੇੈੋੌ੍੎੏ੑ੒੖੗ਖ਼ਗ਼ਜ਼ੜ੝ਫ਼੠੡੢੤੥੦੧੨੩੪੫੬੭੮੯ੰੱੲੳੴੵ੿ંઅઆઇઈઉઋએ‘\n"
     ]
    }
   ],
   "source": [
    "# cleaning the data\n",
    "data = remove_non_punjabi_chars(punj_data)\n",
    "\n",
    "\n",
    "# Getting the vocabulary of characters\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "print(f\"unique_charcters: {''.join(chars)}\")\n",
    "\n",
    "# Character encoding logic\n",
    "stoi = {char:i for i, char in enumerate(chars)}\n",
    "itos = {i:char for i, char in enumerate(chars)}\n",
    "encoder = lambda seq: [stoi[i] for i in seq]\n",
    "decoder = lambda encoding: ''.join([itos[i] for i in encoding])\n",
    "\n",
    "# Encoding the data\n",
    "data = torch.tensor(encoder(data), dtype=torch.long)\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "train, test = data[:int(0.9*len(data))], data[int(0.9*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 70,  94,   1,  ...,  61,  23,   1],\n",
       "         [ 17,   1,  56,  ..., 125,  21,  54],\n",
       "         [  1,  67,  58,  ...,   1,  58,  73],\n",
       "         [ 15,   0,  20,  ...,  58,  72,   1]], device='cuda:0'),\n",
       " tensor([[  0,   1,   2,  ..., 253, 254, 255],\n",
       "         [  0,   1,   2,  ..., 253, 254, 255],\n",
       "         [  0,   1,   2,  ..., 253, 254, 255],\n",
       "         [  0,   1,   2,  ..., 253, 254, 255]], device='cuda:0'),\n",
       " tensor([[94,  1, 38,  ..., 23,  1,  5],\n",
       "         [ 1, 56, 70,  ..., 21, 54,  1],\n",
       "         [67, 58, 60,  ..., 58, 73, 33],\n",
       "         [ 0, 20, 40,  ..., 72,  1, 52]], device='cuda:0'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a sample batch from the data split\n",
    "def get_batch_with_pos(split, batch_size, context_length):\n",
    "    if split == 'train':\n",
    "        data = train\n",
    "    else:\n",
    "        data = test\n",
    "        \n",
    "    #getting random starting indices for the batch_size\n",
    "    start_indices = torch.randint(\n",
    "        len(data) - context_length - 1,\n",
    "        (batch_size,)\n",
    "    )\n",
    "    x_y = torch.stack([data[i:i+context_length+1]for i in start_indices], dim=0)\n",
    "    x, y = x_y[:,:-1], x_y[:,1:]    \n",
    "    pos = torch.arange(batch_size * context_length).reshape(batch_size, context_length) % context_length\n",
    "    x, pos, y = x.to(device), pos.to(device), y.to(device)\n",
    "    return x, pos, y\n",
    "\n",
    "x, pos, y = get_batch_with_pos('train', 4, context_length)\n",
    "x, pos, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, base, dim, max_seq_len):\n",
    "        super(RoPE, self).__init__()\n",
    "        theta = base ** -(torch.arange(0,dim,2)/dim)\n",
    "        pos = torch.arange(max_seq_len)\n",
    "        freq = torch.einsum('i,j->ij', pos, theta)\n",
    "        self.register_buffer('cos', freq.cos())\n",
    "        self.register_buffer('sin', freq.sin())\n",
    "    def forward(self, x):\n",
    "        B, S, _ = x.shape\n",
    "        cos = self.cos[:S]\n",
    "        sin = self.sin[:S]\n",
    "        a, b = x[:,:,::2], x[:,:,1::2]\n",
    "        a_cos, b_cos, a_sin, b_sin = a * cos, b * cos, a * sin, b * sin\n",
    "        # rot(a,b) = a cos(theta) - b sin(theta), a sin(theta) + b cos(theta)\n",
    "        rot_1, rot_2 = a_cos - b_sin, a_sin + b_cos\n",
    "        rot = torch.stack((rot_1, rot_2), -1)\n",
    "        rot_embd = rot.reshape(B, S, -1)\n",
    "        return rot_embd\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFroward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super(FeedFroward, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.query = nn.Linear(n_embd, self.head_dim) #(B,S,C)\n",
    "        self.key = nn.Linear(n_embd, self.head_dim) #(B,S,C)\n",
    "        self.value = nn.Linear(n_embd, self.head_dim) #(B,S,C)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length,context_length)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if rope_embeddings:\n",
    "            self.rope = RoPE(1e4, head_dim, 2048)\n",
    "\n",
    "    def forward(self, embed, verbose=False):\n",
    "        q = self.query(embed)\n",
    "        k = self.key(embed)\n",
    "        v = self.value(embed)\n",
    "        if rope_embeddings:\n",
    "            q = self.rope(q)\n",
    "            k = self.rope(k)\n",
    "        a = q @ k.transpose(-2,-1) * self.head_dim**-0.5\n",
    "        a = a.masked_fill(self.tril==0, float('-inf'))\n",
    "        a = F.softmax(a, dim=-1)\n",
    "        a = self.dropout(a)\n",
    "        if verbose:\n",
    "            print(a.shape)\n",
    "            plt.imshow([[j.item() for j in i]for i in a[0]])\n",
    "\n",
    "        output = a @ v\n",
    "        return output\n",
    "            \n",
    "class MoEMultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, sparsity_factor=0.3):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_size\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "\n",
    "        self.qkv_linear = nn.Linear(n_embd, n_embd * 3)\n",
    "        self.out = nn.Linear(n_embd, n_embd)\n",
    "        self.gate = nn.Linear(n_embd, n_heads, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, hidden_size = x.size()\n",
    "\n",
    "        # Create the keys, queries and values\n",
    "        qkv = self.qkv_linear(x)\n",
    "        #print(qkv.shape)\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
    "        #print(qkv.shape)\n",
    "        qkv = qkv.transpose(1, 2)\n",
    "        #print(qkv.shape)\n",
    "        queries, keys, values = qkv.chunk(3, dim=-1)\n",
    "        #print(queries.shape, keys.shape, values.shape)\n",
    "        # Compute the gate values (scores) for the keys, separate for each head\n",
    "        gate_scores = self.gate(x)\n",
    "        gate_scores = gate_scores.transpose(1, 2)\n",
    "\n",
    "        # Select top-k keys and values based on gate scores\n",
    "        sparsity_count = int(seq_length * self.sparsity_factor)\n",
    "        #print('sparsity_count',sparsity_count)\n",
    "        topk_indices = torch.topk(gate_scores, sparsity_count, dim=-1).indices\n",
    "        topk_indices = topk_indices.unsqueeze(-1).expand(-1, -1, -1, self.head_dim)\n",
    "        #print('topk_indices',topk_indices.shape)\n",
    "        # Gather top-k keys and values\n",
    "        topk_keys = torch.gather(keys, 2, topk_indices)\n",
    "        topk_values = torch.gather(values, 2, topk_indices)\n",
    "\n",
    "        # Compute attention scores and apply softmax\n",
    "        scores = torch.matmul(queries, topk_keys.transpose(-2, -1)) / self.scale\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        #print('Attn', attention.shape)\n",
    "\n",
    "        # Compute context\n",
    "        #print('topk_values', topk_values.shape)\n",
    "        context = torch.matmul(attention, topk_values)\n",
    "        #print('Attn', context.shape)\n",
    "        context = context.transpose(1, 2).reshape(batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.out(context)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for i in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, idx, verbose = False):\n",
    "        output =  torch.cat([head(idx, verbose) for head in self.heads], dim = -1)\n",
    "        output =  self.proj(output)\n",
    "        return self.dropout(output)\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads):\n",
    "        super(Block, self).__init__()\n",
    "        self.mh_attn = MultiHeadAttention(n_heads, n_embd//n_heads)\n",
    "        #self.mh_attn = MoEMultiheadAttention(n_heads, n_embd//n_heads)\n",
    "        self.f_frwd = FeedFroward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self,x):\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mh_attn(x)\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.f_frwd(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class PunjabiAttentionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PunjabiAttentionModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        if not rope_embeddings:\n",
    "            self.position_embedding = nn.Embedding(context_length, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_heads) for i in range(n_layers)])\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length,context_length)))\n",
    "        self.linear = nn.Linear(n_embd, vocab_size)\n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, idx, positions, labels=None, verbose = False):\n",
    "        if verbose:\n",
    "            print([decoder([i.item() for i in idx[0]])],'\\n')\n",
    "        idx = self.token_embedding(idx)\n",
    "        #idx = torch.cat((idx,pos_embed), dim=-1)\n",
    "        if not rope_embeddings:\n",
    "            pos_embed = self.position_embedding(positions)\n",
    "            idx += pos_embed\n",
    "        #idx = self.lm_heads(idx, verbose)\n",
    "        #logits = self.attention(idx, verbose)\n",
    "        idx = self.blocks(idx)\n",
    "        logits = self.linear(idx)\n",
    "        \n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, S, E = logits.shape\n",
    "            #print(labels[0], logits[0])\n",
    "            logits = logits.reshape(B * S, E)\n",
    "            labels = labels.reshape(B*S)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, pos, max_seq_length, sampling=True):\n",
    "        for i in range(max_seq_length):\n",
    "            logits, _ = self(idx[:,-context_length:], pos)\n",
    "            logits = logits[:, -1, :]\n",
    "            if sampling:\n",
    "                probs = F.softmax(logits, -1)\n",
    "                generated_char_ids = torch.multinomial(probs, 1)\n",
    "                idx = torch.cat((idx, generated_char_ids),dim=1)\n",
    "            else:\n",
    "                generated_char_ids = logits.argmax(-1)\n",
    "                idx = torch.cat((idx, generated_char_ids.unsqueeze(0).T),dim=1)\n",
    "        return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # to tell pytorch to not store intermediate variables as we won't do back propagation in the function\n",
    "def evaluate_attn(batch_size, model):\n",
    "    model.eval()\n",
    "    losses = {}\n",
    "    for split in ['train', 'eval']:\n",
    "        x, pos, y = get_batch_with_pos(split, batch_size, context_length)\n",
    "        _, loss = model(x, pos, y)\n",
    "        losses[split] = loss.item()\n",
    "    return losses\n",
    "\n",
    "\n",
    "model_attn = PunjabiAttentionModel()\n",
    "model_attn.to(device)\n",
    "optimizer_attn = torch.optim.AdamW(model_attn.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 4.8726606369018555, eval_loss: 4.867763042449951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 501/5000 [01:29<25:50,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.716081142425537, eval_loss: 1.7225172519683838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1001/5000 [02:58<22:58,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.5680975914001465, eval_loss: 1.574684500694275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1501/5000 [04:26<20:06,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.4988981485366821, eval_loss: 1.5263890027999878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2001/5000 [05:55<17:13,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.4350512027740479, eval_loss: 1.449171543121338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2501/5000 [07:23<14:21,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.3940855264663696, eval_loss: 1.4107813835144043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3001/5000 [08:52<11:29,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.372545838356018, eval_loss: 1.3732761144638062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3501/5000 [10:20<08:36,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.324541687965393, eval_loss: 1.3378702402114868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4001/5000 [11:49<05:44,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.2839099168777466, eval_loss: 1.3419642448425293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4501/5000 [13:17<02:52,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.284499168395996, eval_loss: 1.304429054260254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [14:45<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2776238918304443\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(max_iters)):\n",
    "    if i % eval_interval == 0:\n",
    "        losses = evaluate_attn(batch_size = eval_iters, model = model_attn)\n",
    "        print(f'train loss: {losses[\"train\"]}, eval_loss: {losses[\"eval\"]}')\n",
    "    x, pos, y = get_batch_with_pos('train', batch_size, context_length)\n",
    "    _, loss = model_attn(x, pos, y)\n",
    "    optimizer_attn.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_attn.step()\n",
    "print(loss.item())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ਪੰਜਾਬ ਸਰਕਾਰ\n",
      "\n",
      "generation:  ਵਲੋਂ ਜਨਵਰੀ 2058 ਪੰਜਾਬੀ ਫੌਜ ਨੇ ਸਿਲੈਰਿੰਗ ਅਤੇ ਕੋਚ ਵਿਚ ਪੰਚਾਇਤ ਸੰਮਤੀ ਨਾਲ ਸਬੰਧਤ ਉਨੇਗਾ ਜਾਰੀ \n",
      "ਆਲੋਚਕਾਂ ਦੀ ਸਲਾਹ ਬਿੰਦੂ ? ਦੁਬਈ ਤੋਂ ਮੁਕਤੀ ਬਾਅਦ ਵੀ ਇਹ ਵਾਕ ਬੈਚ ਸ਼ੇਰ ਮਾਮਲੇ ਵਿਚ ਕਰਵਾਇਆ ਸ਼ਿਕਾਇਤਾਂ ਨੂੰ ਛੁਡਾਊ ਲਈ ਯੂਕੇ ਵੱਲੋਂ ਯਾਦ ਕਰਾਇਆ ਗਿਆ ਕਿ ਇੰਟਰਨੈੱਟ ਵੱਲੋਂ ਜਨਰੇਟਰ ਵੀ ਤਿਆਗੇ ਟੂਰਾਂ ਲਈ ਕਹੀ ਗਰਮ ਜਾਗਿੰਗ ਸ਼ਾਮਲ ਸੋਨਾ\n",
      "ਦੇਸ਼ ਪਿੰਡ ਸ਼ਿਅਨਦ ਵਿਖੇ ਸੈੱਟ ਖੱਬੇ ਦੇ ਵਿਚਾਰਕ ਸ਼ਿਲਾਂਗ ਵਿਚ ਸ਼ਹਿਰ ਦੇ ਹੋਰਨਾਂ ਟੁੱਟੀਆਂ ਹਨ ਤੇ ਬੋਲਦਿਆਂ ਖੁਦਕੁਸ਼ੀਆਂ ਦਾ ਕਹਿਣਾ ਹੈ ਕਿ ਕੀ ਬੋਲੇ ਵਿਖਾਵੇ ਸਿਵਰ ਹੁੰਦੇ ਹਨ, ਜਿਨ੍ਹਾਂ ਸ਼ਿਕਾਇਤਾਂ ਤੇ ਟਿਕਾਣੇ ਦੇ ਬਾਅਦ ਜਿੱਤ ਨਾਲ ਪ੍ਰਾਪਤ ਕਰਨਾ ਚਾਹੁੰਦੇ ਹਨ ਜਦੋਂ ਮੁੜ \n"
     ]
    }
   ],
   "source": [
    "def generate_text(context, model, gen_len, sampling=True):\n",
    "    pad = ''.join([' ' for i in range(context_length - len(context))])\n",
    "    padded_context = pad + context\n",
    "    x = torch.tensor([encoder(padded_context)], device = device)\n",
    "    pos = torch.arange(context_length).unsqueeze(0)\n",
    "    pos = pos.to(device)\n",
    "    output = model_attn.generate(x,pos, gen_len, sampling)\n",
    "    print(f'context: {context}\\n')\n",
    "    generation = decoder([i.item() for i in output[0][-gen_len:]])\n",
    "    print(f'generation: {generation}')\n",
    "    return generation\n",
    "\n",
    "\n",
    "context = 'ਪੰਜਾਬ ਸਰਕਾਰ'\n",
    "gen = generate_text(context, model_attn, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### producing deterministic output, without sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ਪੰਜਾਬ ਸਰਕਾਰ\n",
      "\n",
      "generation:  ਵੱਲੋਂ ਕੀਤਾ ਗਿਆ ਹੈ ਕਿ ਉਹ ਕਿਸੇ ਵੀ ਕਿਸੇ ਨੂੰ ਕਿਸੇ ਵੀ ਕਿਸੇ ਨੂੰ ਕਿਸੇ ਨਾਲ ਕਿਸੇ ਨਾਲ ਕਿਸੇ ਨੂੰ ਕਿਸੇ ਨਾਲ ਕਿਸੇ \n"
     ]
    }
   ],
   "source": [
    "gen = generate_text(context, model_attn, 100, sampling = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ਪੰਜਾਬ ਦੀਆਂ ਚੋਣਾਂ ਜਿੱਤੀਆਂ ਸਨ\n",
      "\n",
      "generation:  ਜ਼ਿੱਦ ਕੂੜੀਆਂ ਟਿੱਪਣੀਆਂ ਹੋ ਚੁੱਕੇ, ਉਸਨੇ ਸ਼ੁਰੂ ਕੀਤਾ ਕਿ \n",
      "ਪੇਸ਼ਕਦਮੀ ਦਾ ਮੁੱਖ ਮੰਤਰੀ ਹੋ ਕੇ ਤਖ਼ਤ ਹੋ ਕੇ ਸਬੰਧੀ ਸੜਕ ਤੇ ਹੋਏ ਕਿਹਾ ?\n",
      "ਪਾਣੀ ਬੰਦ \n",
      "ਬਈ ਐਸ ਐਸ ਦੀ ਵਰਡ ਕੁਲਵੰਤ ਸਿੰਘ ਨੇ ਸੀਰੀਆ ਤੇ ਮੀਡੀਆ ਐਸੋਸੀਏਸ਼ਨ ‘ਚ ਖੋਜ਼ ਜ ਮੋਦੀ ਭਗਵੰਤ ਮਾਨ ਲਗਾਉਣ ਨਾਲ ਸੋਸਾਇਟੀ ਦੀ ਆਦਤ ਨਹੀਂ ਕਰ ਲਿਆ \n",
      "ਰਾਇਸ ਅਕਾਲੀ ਦਲ ਦੇ ਵਿਚਕਾਰ ਐਲਾਨ ਕੀਤਾ ਜ਼ਿੰਦਾ \n",
      "ਟ੍ਰੇਨਰਾਂ ਨੂੰ ਟਰੂਡੋ ਕਾਲਾ ਧਨ ਦਾ ਦਾਖ਼ਲਾ ਦਰਜ ਕਰਵਾਇਆ\n",
      "ਨਵਾਂ ਸਾਲਾਨਾ ਆਮੀਨੀ ਜੀਵਨ ਦੀਆਂ ਨਵੀਂਆਂ ਕਿਸਮਾਂ 21 ਸਤੰਬਰ 1982 ਦੇ ਰਾਖੇ ਚੋਰੀ ਨਸਲਕੁਸ਼ਾ ਸੜਕ ਹਾਦਸੇ ਪੰਜਾਬ ਦਿਵਸ ਵੱਲੋਂ ਤਸਵੀਰ ਕੌਰ ਪੰਜਾਬ ਵਲੋਂ ਖੁਸ਼ੀ ਪਵਾਏ, ਅਜੈਬਲੀ ਅਤੇ ਸੇਦਬੁਰ ਫਰੇਲਰ ਤੋਂ ਓਜਲ ਨਾਲ\n"
     ]
    }
   ],
   "source": [
    "context = 'ਪੰਜਾਬ ਦੀਆਂ ਚੋਣਾਂ ਜਿੱਤੀਆਂ ਸਨ'\n",
    "gen = generate_text(context, model_attn, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'model/punjabi_rope_5k.pth'\n",
    "torch.save(model_attn.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PunjabiAttentionModel(\n",
       "  (token_embedding): Embedding(126, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (mh_attn): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x AttentionHead(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (rope): RoPE()\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (f_frwd): FeedFroward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (mh_attn): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x AttentionHead(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (rope): RoPE()\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (f_frwd): FeedFroward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (mh_attn): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x AttentionHead(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (rope): RoPE()\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (f_frwd): FeedFroward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (mh_attn): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x AttentionHead(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (rope): RoPE()\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (f_frwd): FeedFroward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (mh_attn): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x AttentionHead(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (rope): RoPE()\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (f_frwd): FeedFroward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (mh_attn): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x AttentionHead(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (rope): RoPE()\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (f_frwd): FeedFroward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=384, out_features=126, bias=True)\n",
       "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = PunjabiAttentionModel()\n",
    "model_loaded.load_state_dict(torch.load(path))\n",
    "model_loaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ਅੱਜ ਦੀ ਖਬਰ\n",
      "\n",
      "generation:  ਬਾਦਲ ਅਤੇ ਉਨ੍ਹਾਂ ਨੂੰ ਪੁਨਰਸਨਤਾ ਦੀ ਆਗਿਆ ਤੇ ਉਨ੍ਹਾਂ ਨੂੰ ਬਾਪੂ ਨਾਲ ਗੱਲ ਕਰਦੇ ਹਨ। ਆਪਣੀ ਮੁਟਿਆਰਸ਼ਾਂ ਦੇ ਸੰਤ ਅੱਤਵਾਦੀ ਅੰਦਰ ਪੁਸਤਕ ਦੇ ਮੁਖੀ ਨੂੰ ਤਰੱਕ ਕੇ ਸਮਾਜ ਰਾਹੀ, ਨਸ਼ਿਆਂ ਦੇ ਮੋਹ ਹੈ ਜਿਨ੍ਹਾਂ ਦੇ ਭਿਖੇਰੇ ਇਟਾਲੀਅ ਦੀ ਨਿਵਾਸੀ ਸਮੇਂ ਸਹੂਲਤ ਸਭਨਾ ਨਿਭਾਉਣ ਲਈ ਇਸ ਤੋਂ ਬਾਅਦ ਉਨ੍ਹਾਂ ਤਕ ਅਸਲਰੋਨ ਸਰੀਰ ਵਲੋਂ ਆਤਮਾ ਨਾਲ ਸੂਬਦ ਦੀਆਂ ਛਪੀਆਂ ਖਰੀਆਂ ਕਿਤਾਬ, ਅਤੇ ਕਿਸੇ ਪਾਣੀ ਦੀ ਯਮਾਤਾ ਅਤੇ ਸਬੂਹਾਂ ਤੇ ਬੌਸ ਲਿਖਤ ਪੁਸਤਕਾਂ ਤੱਕ ਹਨ ਜਿਸ ਨੂੰ ਪੈਸੇ ਦਾ ਅਸਾਧਾਰਨ ਕੀਤਾ ਜਾ ਸਕਦੇ ਸਨ ਉਨ੍ਹਾਂ ਦਾ ਸ਼ਿਕਾਰ ਹਵਾਈ ਸ਼ੁਰੂ ਕੀਤਾ ਹੈ, ਆਈਐਸ ਇੰਡੋ , ਜ਼ਬੀਰ ਸਿੰਘ ਖੁਸ਼ੀ ਦੇ ਸਮੇਂ ਅਤੇ ਹਾਇਰਨ ਵੱਲੋਂ ਸਰਕਾਰ ਵਲੋ \n"
     ]
    }
   ],
   "source": [
    "gen = generate_text('ਅੱਜ ਦੀ ਖਬਰ', model_attn, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5000 [00:00<58:21,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.2834171056747437, eval_loss: 1.2880988121032715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 501/5000 [01:29<25:46,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.256445288658142, eval_loss: 1.2932637929916382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1001/5000 [02:57<22:58,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.233232021331787, eval_loss: 1.25784170627594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1501/5000 [04:25<20:05,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.211179494857788, eval_loss: 1.2314311265945435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2001/5000 [05:54<17:12,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.2695534229278564, eval_loss: 1.2136850357055664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2501/5000 [07:22<14:22,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.2127000093460083, eval_loss: 1.2186024188995361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3001/5000 [08:51<11:29,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.214529275894165, eval_loss: 1.2080219984054565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3501/5000 [10:19<08:36,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.2092314958572388, eval_loss: 1.2074851989746094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4001/5000 [11:48<05:44,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.2133549451828003, eval_loss: 1.192925214767456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4501/5000 [13:16<02:51,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.1670435667037964, eval_loss: 1.187902569770813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [14:44<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.209726095199585\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(max_iters)):\n",
    "    if i % eval_interval == 0:\n",
    "        losses = evaluate_attn(batch_size = eval_iters, model = model_attn)\n",
    "        print(f'train loss: {losses[\"train\"]}, eval_loss: {losses[\"eval\"]}')\n",
    "    x, pos, y = get_batch_with_pos('train', batch_size, context_length)\n",
    "    _, loss = model_attn(x, pos, y)\n",
    "    optimizer_attn.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_attn.step()\n",
    "print(loss.item())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'model/punjabi_rope_10k.pth'\n",
    "torch.save(model_attn.state_dict(), path)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ਪੰਜਾਬ ਚਾਹੁੰਦਾ ਹੈ\n",
      "\n",
      "generation: । ਬਾਓਸਟੇਰੀਆ ਦੀ ਵੱਡੀ ਹਾਈਕਮਾਂਡ ਲਉ ਤੁਹਾਨੂੰ ਹਰ ਹਰਿਆਣਾ ਬਾਰੇ ਖ਼ੁਦ ਕਰਨ ਲਈ ਥਾਂ ਲਿਖਿਆ ਹੈ?\n",
      "45 ਦਿਨ ਦਾ ਪਰਬੰਧ ਕਰਨਾ ਸ਼੍ਰੋਮਣੀ ਕਮੇਟੀਆਂ ਦੇ ਪਰਿਵਾਰ ਦੀ ਟਹਿਣੀ ਤੇ ਪੈਦਂ ਥੱਲਾ ਸ਼ਹਰਤ ਦੀ ਟਹਿਣੀ ਮਲਿਕਦਾਰ ਹੋਣਾ ਚਾਹੀਦਾ ਹੈ।\n",
      "ਸਭ ਕੁਝ ਵੀ ਹੈ ਜੋ ਹਿਰਦੈ ਪ੍ਰੋਵੇਕ ਗੁਰਬਾਣੀ ਹੈ। ਇਸ ਲਈ ਜ਼ਿਲ੍ਹਾ ਕਲਫ਼ਿਆਂ ਦੇ ਗਿਅਣ ਨਾ ਜਿਈਏ ਕਹਿਸ਼ ਤੇ ਵਰਦੇ ਵੀ ਛਿਲੈ ਪਰਾਪਤ ਕਰਨ ਨਾਲ ਸਿੱਖਾਂ ਵਾਂਗ ਇਸ ਵੈਬਸ਼ਨ ਅੰਦਰੋਂ ਪੈਦਾ ਕੀਤੇ ਗਏ। ਪਰਿਵਾਰਾਂ ਨੂੰ ਅੱਗੇ ਪਰਵਾਰ ਦੇ ਚੱਕਰ ਦੇ ਨਾਲ ਇੱਟਾਂ ਤੋਂ ਹਟਾ ਕੇ ਖੇਮਾ ਦੇ ਪਰਵਾਨ ਕੀਤੇ ਗਏ। ਜਿਹੜੇ ਬੰਦੇ ਮੇਲਿਆਂ ਵਾਲਿਆਂ ਲਈ 6 ਬਾਲੀਵੁੱਡ ਵਿਚ ਸੈਟੇਲਫਾਈਟ ਸ੍ਟੋਰੀਟਸ ਵਾਲਿਆਂ ਦੇ ਸਭ ਹੁਣ \n"
     ]
    }
   ],
   "source": [
    "gen = generate_text('ਪੰਜਾਬ ਚਾਹੁੰਦਾ ਹੈ', model_attn, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ਭਾਰਤ ਚਾਹੁੰਦਾ ਹੈ\n",
      "\n",
      "generation: \n",
      "ਬਹੁਤ ਵਾਇਰਲ ਹੈ ਜੋ ਸਟਿਮ ਦਿਨ ਡਿਜੀਟਲ ਬਣਾਉਂਦਾ ਹੈ। ਹਰੇਕ ਜੀਪ\n",
      "ਪੱਤਰਕਾਰ ਨੂੰ ਉਚਿਤ ਪਹਿਲੀ ਹੀ ਆਨੰਦ ਦੇ ਕੇ ਹੱਥ ਹੋਵੇਗਾ ਕਿ ਪਾਰਕੀ ਦੇ ਰੋਹਣੇ ਰੋਂਦੀ ਹੈ ਜਾਂ ਏਡਿਜੀਟਲ ਦੀ ਕਥਿਤ ਦੋਸਤਾਨਾਂ ਦੀ ਚਾਵਲ ਹੈ। \n",
      "ਸਟਿੱਕ ਵੀ ਨੋਟ ਕ੍ਰਿਸਟੋਜਨਫਾਰਮੇ ਚ ਰਾਮਗੜ੍ਹ ਚ 19 ਅੱਜ ਤੋਂ\n",
      "ਇਸਲਾਮਾਬਾਦ ਪਾਕਿਸਤਾਨ ਘਨੌਰ ਨੇ 1989 ਵਿੱਚ ਇੰਦੌਰ ਮਿਊਜ਼ਰ 29 1970 ਇੰਚ ਦੀਆਂ ਅਰਬਾਂ ਖੇਡ ਚਲਦੇ ਰਹਿਣ। ਪਹਿਲੀ ਵਾਰ ਉਹ 92 99 ਇੰਚ 18, 998 \n",
      "ਓਸਾਮਾ ਬਿਨ ਲਾਦਰੇ\n",
      "ਅਕਸ਼ੇ ਕਾਮਿਆਂ ਦਾ ਨਾਂ ਖਾਣਾ ਘਟਾਉਣਾ ਚੋਣ ਪ੍ਰਕਿਰਿਆ\n",
      "1941 ਤੋਂ ਘਟਾਉਣ ਵਾਲੀ ਔਕੜ ਚ ਲੈਕੇ \n",
      "1974 ਕਤਲੇਆਮ ਸੜਕ ਤੇ ਦੋਸ਼ੀ ਇਜ਼ਰਾਇਲੀ ਜੋ ਰਾਹੀਂ ਜੀ ਆਇਆਂ ਕੋਲ਼ਾ ਵਿਚ ਅਕਸ਼\n"
     ]
    }
   ],
   "source": [
    "gen = generate_text('ਭਾਰਤ ਚਾਹੁੰਦਾ ਹੈ', model_attn, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ਭਾਰਤ ਦੀ ਹਾਕੀ ਟੀਮ\n",
      "\n",
      "generation:  ਦਾ ਸੰਤੁਜੇ ਹਾਸਲ ਕਰਨ ਵਾਲਿਆਂ ਤੇ ਬਾਲਾ ਦੀ ਹਾਲਤ ਮਾਰ ਕੇ ਰੱਖੀਂ ਹੀਰੋ ਸਾਹਿਬ ਭਰੇ ਨਵ ਏਜੰਡਾ ਰਾਮ ਕੁਮਾਰ ਦੀ ਥੂਹੀ\n",
      " 21, 2018ਟਸ। ਪੋਸਟ ਬਿਊਰੋ ਆਪਣੀ ਸੂਚੀ ਦੇ ਨਾਲ ਯੋਗ ਅੱਗੇ ਗੂੜ੍ਹੇ ਦੇ ਰੰਗ ਦੀ ਨਾਵਲ, ਅਣ ਲੋਕ ਗਾਲੀ ਪੁੱਤਰ ਸਾਹਿਬ ਨੇਤ੍ਰਤਾ ਨਾਲ ਸੰਸਥਾ ਦੇ ਮਾਰਵਿਨ ਅਾਵੇ ਭਾਰਤੀ ਮੂਲ ਦਾ ਜਨਮ ਦਿਨ ਲਗਾਇਆ ਸੀ। ਉਨ੍ਹਾਂ ਉਨ੍ਹਾਂ ਦੇ ਇੱਕ ਹਸਪਤਾਲ ਚ ਉਨ੍ਹਾਂ ਦਾ ਯੋਗਦਾਨ \n",
      "ਕਰੀਬ ਇੱਕ ਝਲਕ ਦੀ ਇੱਕ ਵਾਰ ਮੈਡਮ ਬੰਦੇ ਦੀ ਪਨੀਤੀ ਘੁਟੋਆ। ਪ੍ਰਧਾਨ ਮੰਤਰੀ\n",
      "300 ਗੱਲ ਬਾਤ ਯੋਗਦਾਨ ਨਾਮੋ ਬਾਤ ਚਿਨ੍ਹਾਂ ਨੇ ਕਲੱਬ ਲੋਹੇਸ਼ ਦਾ ਮੋਬਾਇਲ ਚਾਰਦੀ ਨਾਲ ਕੀਤਾ। ਬਤੌਰ ਬਾਰਿਸ਼ ਦੀ ਖ਼ੁਰਾਕ ਘਟੀਆ ਮਾਰਚ ਕੀਤੀ ਆਮਦਨ ਸੰਤਾ, ਮਾਹਿਸਾ ਦ\n"
     ]
    }
   ],
   "source": [
    "gen = generate_text('ਭਾਰਤ ਦੀ ਹਾਕੀ ਟੀਮ', model_attn, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ਕ੍ਰਿਕਟ\n",
      "\n",
      "generation:  ਸਵਾਰ ਐਥੇਨ ਨੌਮ 857 ਹੋਰ ਮੰਚਲ ਸ਼੍ਰੋਮਣੀ ਗੁਰਦੁਆਰੇ ਦੇ ਸੂਬਾ ਪ੍ਰਧਾਨ ਚੁਣੇ ਗੰਢੀ, ਤਹਿਸੀਲ ਏਅਰ ਏਚੀਟ ਦੇ ਚੇਅਰਮੈਨ, ਕੈਡਕੀ ਵਾਰਸਭਿਅਤਾ ਅਤੇ ਪੂਰਬੀ ਏਸ਼ੀਆ ਚ ਮੌਜੂਦਾ ਪਾਡ਼ਾਸਿਨਰਸਿਪੀ, ਜਥੇਦਾਰ ਅਵਤਾਰ ਸਿੰਘ ਬੱਸੜ ਬਲਵੀਪੁਰ ਦੇ ਲੰੈਕਿਸੇਆਂ ਦੇ ਬਸਤੀ ਚੇਅਰਮੈਨ ਜਗਰਾੳਾਣ ਕਰਦੇ ਹਨ, ਫਿਰ ਉਨ੍ਹਾਂ ਦਿਨੇਸ਼ ਅਤੇ ਸੱਭਿਆਚਾਰਾਂ ਨੂੰ ਨਹੀਂ ਮਿਲਿਆ ਹੈ।\n",
      " ਅਕਾਲੀ ਦਲ ਭਾਜਪਾ ਦੇ ਸੂਬਾ ਕੌਂਸਲ ਏਜੰਸੀ ਕੈਪਟਨ ਸਾਰੀਆਂ ਵਿਕੀਪੀਡੀਆ, ਇਕ ਅਜ਼ਾਦੀ ਘਟਨਾ ਅਤੇ ਇਨ੍ਹਾਂ ਨੂੰ ਹੁੰਦਿਆਂ ਚੁਣਿਆ ਗਿਆ ਹੈ। ਕੈਥਲੀਨ ਦੀ ਵਿਦਿਅਕ ਨਈਟਸ ਹਾਂ, ਇਹੀ ਇਸ ਤੋਂ ਪਹਿਲਾਂ ਉਨ੍ਹਾਂ \n",
      "ਇੰਨਸਾਫ਼ ਨਾਲ ਸਬੰਧਤ ਮੰਤਰੀ ਨਾਲ ਮੋਹਾਲੀ ਦੀ ਬੇਨਤੀ ਰੀਅਲਜੁਦਾ \n"
     ]
    }
   ],
   "source": [
    "gen = generate_text('ਕ੍ਰਿਕਟ', model_attn, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ਮੰਤਰੀ\n",
      "\n",
      "generation:  ਬਣਕੇ ਹੋਣ ਦਾ ਉਪਾਅ ਦਿੱਤਾ।ਹੇਠਾਂ ਦਸੇ ਸਾਰੇ ਵਿਚਾਰੇ ਪੁੱਤਰ ਪਰਮੀਸ਼ ਦੀ ਹੈ ਜੋ ਵਿਆਹ ਬੈਠਣ ਦਾ ਵੀ ਚਾਰ, ਵਾਰਾਂ ਦੀ ਜੀਵਨ ਮਰਜ਼ੀ ਨਾਲ ਆਈ ਕਿਸਾਨੀ ਦਾ ਫ਼ੈਸਲਾ ਲੈਣ ਦਾ ਸਹਾਈ ਜਿਹੜਾ ਕੁਦਰਤ ਬਣੇ ਹੋਏ, ਉਹ ਆਖ਼ਰੀ ਪੁਲੇਸ ਚਾਰੇ ਤੇ ਰਿਸਕਾਰੀ। ‘ਹਾਸਰੀਆਂ ਨੇ ਛੇੜੀ ਜਾ ਉਪਰੇਸ਼ਨ ਕੀਤੀ, ਨੇ ਤੱਕੇ, ਕਿਹਨੂੰ ਧੂਫੀ ਵਿਰੁੱਧ ਅਥਵਾ ਪਾਉਣਾ। ਆਖ਼ਰੀ ਦੀ ਕਮੀ ਹਰੇਕ ਕੋਈ, ਟੱਪ ਰਿਹਾ। ਜੀ ਫੜਨਾ ਲੱਗਦੈ ਹੁੰਦਾ ਹੈ ਅਸੀਂ ਬੇਚੈਨੀ ਭੇਜ ਕੇ ਅੰਦਰਿਆਂ ਚ ਆਉਂਦੇ ਰਹੇ ਝਿੜਕੇ ਤੇ ਸਿਰਫ\n",
      "ਖ਼ੁਦ, ਇਕ ਤਰਣਾ ਸਵਾਲ ਵੀ ਨਾ ਹੋਈ ।। ਇੱਕ ਦੂਜੇ ਦੇ ਅੰਦਰ ਆਕਾਸ਼ ਉਪਰ ਢੰਗ ਰਿਹਾ ਤੋਂ ਆਵੇ, ਪਹਿਲੇਵਾਂ। ਸ਼ਾਜ ਦਾ ਪਿੰਡ ਰਬੂਬਲ ੱਲਾਣਾ ਚਾਹੁੰਦੀ ਏ।\n",
      "ਰਾਜਦੇ\n"
     ]
    }
   ],
   "source": [
    "gen = generate_text('ਮੰਤਰੀ', model_attn, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
