{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_punjabi_chars(text):\n",
    "    punjabi_chars = r\"[\\u0A01-\\u0A7F\\u0A80-\\u0A8F,।0-9? \\n‘:]\"  # Gurmukhi range\n",
    "    english_chars = r\"[a-zA-Z]\"  # English alphabet range\n",
    "    text =  re.sub(r\"[^\" + punjabi_chars +\"|\"+ english_chars + \"]+\", \" \", text) \n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pa.txt') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = remove_non_punjabi_chars(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clean_pa.txt', 'w') as f:\n",
    "    f.write(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 1.31 s, total: 1min 7s\n",
      "Wall time: 36.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/clean_pa.txt\n",
      "  input_format: \n",
      "  model_prefix: pure_punjabi_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 1000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc_cf\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: ../data/clean_pa.txt\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(143) LOG(INFO) Loaded 3000000 lines\n",
      "trainer_interface.cc(408) LOG(INFO) Sampled 1000000 sentences from 3814215 sentences.\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=88288426\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.954% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=73\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.99954\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 1000000 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 388865 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 1000000\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 298739\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 298739 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=154930 obj=10.4256 num_tokens=614041 num_tokens/piece=3.96334\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=127998 obj=8.59676 num_tokens=608572 num_tokens/piece=4.75454\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=95972 obj=8.54339 num_tokens=630633 num_tokens/piece=6.57101\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=95883 obj=8.53651 num_tokens=630560 num_tokens/piece=6.57635\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=71908 obj=8.55466 num_tokens=669513 num_tokens/piece=9.31069\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=71906 obj=8.55013 num_tokens=669487 num_tokens/piece=9.31059\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=53929 obj=8.58057 num_tokens=713301 num_tokens/piece=13.2267\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=53929 obj=8.57394 num_tokens=713285 num_tokens/piece=13.2264\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=40446 obj=8.61907 num_tokens=758661 num_tokens/piece=18.7574\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=40446 obj=8.60952 num_tokens=758651 num_tokens/piece=18.7571\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=30334 obj=8.6781 num_tokens=804353 num_tokens/piece=26.5165\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=30334 obj=8.66413 num_tokens=804361 num_tokens/piece=26.5168\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=22750 obj=8.75605 num_tokens=850637 num_tokens/piece=37.3906\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=22750 obj=8.73673 num_tokens=850667 num_tokens/piece=37.392\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=17600 obj=8.84469 num_tokens=891373 num_tokens/piece=50.6462\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=17600 obj=8.82277 num_tokens=891413 num_tokens/piece=50.6485\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: pure_punjabi_tokenizer.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: pure_punjabi_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='../data/clean_pa.txt',\n",
    "    model_prefix='pure_punjabi_tokenizer',  \n",
    "    model_type='unigram',\n",
    "    vocab_size=16000,  # Adjust based on your corpus size and language complexity\n",
    "    character_coverage=0.9995,\n",
    "    input_sentence_size=1000000,\n",
    "    shuffle_input_sentence=True,\n",
    "    normalization_rule_name='nmt_nfkc_cf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16000\n",
      "Sample tokens:\n",
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      ",\n",
      "▁ਦੇ\n",
      "।\n",
      "▁ਦੀ\n",
      "▁ਹੈ\n",
      "▁ਨੂੰ\n",
      "▁ਦਾ\n",
      "▁ਨੇ\n",
      "▁ਅਤੇ\n",
      "▁ਤੇ\n",
      "▁ਵਿੱਚ\n",
      "▁ਲਈ\n",
      "▁ਨਾਲ\n",
      "▁ਤੋਂ\n",
      "▁‘\n",
      "▁ਸਿੰਘ\n",
      "▁ਇਸ\n",
      "▁ਹਨ\n",
      "▁ਚ\n",
      "▁ਕਿ\n",
      "▁ਵਿਚ\n",
      "▁ਕੇ\n",
      "▁ਵੀ\n",
      "▁ਸੀ\n",
      "ੀ\n",
      "▁ਨਹੀਂ\n",
      "▁ਕਰਨ\n",
      "▁ਇਹ\n",
      "▁\n",
      "ਚ\n",
      "▁ਹੋ\n",
      "ਾਂ\n",
      ":\n",
      "▁ਹੀ\n",
      "▁ਇੱਕ\n",
      "▁ਕਰ\n",
      "▁:\n",
      "▁ਪੰਜਾਬ\n",
      "▁ਕੀਤਾ\n",
      "▁ਉਹ\n",
      "▁ਉਸ\n",
      "ਾ\n",
      "▁ਤਾਂ\n",
      "▁ਗਿਆ\n",
      "▁ਜੀ\n",
      "▁ਆਪਣੇ\n",
      "ੇ\n",
      "ਤੇ\n",
      "▁ਇਕ\n",
      "▁ਵੱਲੋਂ\n",
      "▁ਦੀਆਂ\n",
      "▁ਕੀਤੀ\n",
      "▁ਜੋ\n",
      "▁,\n",
      "▁ਉਨ੍ਹਾਂ\n",
      "▁ਕਰੋ\n",
      "▁ਪਰ\n",
      "▁ਗੁਰੂ\n",
      "▁ਨਾ\n",
      "▁ਹੋਰ\n",
      "▁ਸਾਹਿਬ\n",
      "▁ਜਾ\n",
      "▁ਵਾਲੇ\n",
      "?\n",
      "▁ਕੋਈ\n",
      "▁ਰਹੇ\n",
      "▁ਸਰਕਾਰ\n",
      "▁ਮੰਤਰੀ\n",
      "▁ਵੀਡੀਓ\n",
      "▁ਮੈਂ\n",
      "▁ਕੀ\n",
      "▁ਗਈ\n",
      "▁ਕਿਹਾ\n",
      "▁ਹੋਏ\n",
      "▁।\n",
      "▁ਅੱਜ\n",
      "▁ਗਏ\n",
      "▁ਵਲੋਂ\n",
      "▁ਜਿਸ\n",
      "ਨ\n",
      "▁ਪੰਜਾਬੀ\n",
      "▁ਦਿੱਤਾ\n",
      "▁ਬਾਰੇ\n",
      "▁ਭਾਰਤ\n",
      "▁ਲੋਕਾਂ\n",
      "▁ਸਿੱਖ\n",
      "▁ਵਿਖੇ\n",
      "▁ਰਿਹਾ\n",
      "▁ਤੱਕ\n",
      "▁ਹੋਣ\n",
      "▁ਹਾਂ\n",
      "▁ਹੋਈ\n",
      "ਸ\n",
      "▁ਦਿਨ\n",
      "▁ਕਿਸੇ\n",
      "▁ਸਭ\n",
      "▁ਹੁਣ\n",
      "Tokenized: ['▁ਪੰਜਾਬੀ', '▁ਇੱਕ', '▁ਬਹੁਤ', '▁ਸੁੰਦਰ', '▁ਭਾਸ਼ਾ', '▁ਹੈ']\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('pure_punjabi_tokenizer.model')\n",
    "\n",
    "# Print vocabulary size\n",
    "print(f\"Vocabulary size: {sp.get_piece_size()}\")\n",
    "\n",
    "# Print some tokens\n",
    "print(\"Sample tokens:\")\n",
    "for i in range(100):  # Print first 100 tokens\n",
    "    print(sp.id_to_piece(i))\n",
    "\n",
    "# Try tokenizing a Punjabi sentence\n",
    "punjabi_sentence = \"ਪੰਜਾਬੀ ਇੱਕ ਬਹੁਤ ਸੁੰਦਰ ਭਾਸ਼ਾ ਹੈ\"\n",
    "tokens = sp.encode_as_pieces(punjabi_sentence)\n",
    "print(f\"Tokenized: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punjabi text: ਾਲ ਮੁੱਦੇ ‘ਤੇ ਪਹਿਲੀ ਵਾਰ ਸਾਹਮਣੇ ਆਇਆ ਪੀ. ਐੱਮ. ਟਰੂਡੋ ਦੀ ਪਤਨੀ ਸੋਫੀ ਦਾ ਬਿਆਨ\n",
      "Comments Off on ਅਟਵਾਲ ਮੁੱਦੇ ‘ਤ\n",
      "Tokenized: ['▁', 'ਾਲ', '▁ਮੁੱਦੇ', '▁‘', 'ਤੇ', '▁ਪਹਿਲੀ', '▁ਵਾਰ', '▁ਸਾਹਮਣੇ', '▁ਆਇਆ', '▁ਪੀ', '.', '▁ਐੱਮ', '.', '▁ਟਰੂਡੋ', '▁ਦੀ', '▁ਪਤਨੀ', '▁ਸੋ', 'ਫੀ', '▁ਦਾ', '▁ਬਿਆਨ', '▁', 'comments', '▁', 'off', '▁', 'on', '▁ਅਟਵਾਲ', '▁ਮੁੱਦੇ', '▁‘', 'ਤ']\n"
     ]
    }
   ],
   "source": [
    "ind = random.randint(0, len(data)-100)\n",
    "punj_text = data[ind:ind+100]\n",
    "\n",
    "print(f'Punjabi text: {punj_text}')\n",
    "\n",
    "tokens = sp.encode_as_pieces(punj_text)\n",
    "print(f\"Tokenized: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "573",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
